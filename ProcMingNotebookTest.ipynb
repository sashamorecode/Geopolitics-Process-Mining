{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import pandas\n",
    "from enum import Enum\n",
    "from pm4py.objects.conversion.log import converter\n",
    "from pm4py.objects.log.util import sorting\n",
    "\n",
    "from pm4py.util import exec_utils, xes_constants, constants\n",
    "\n",
    "class Parameters(Enum):\n",
    "    ACTIVITY_KEY = constants.PARAMETER_CONSTANT_ACTIVITY_KEY\n",
    "    TIMESTAMP_KEY = constants.PARAMETER_CONSTANT_TIMESTAMP_KEY\n",
    "    START_TIMESTAMP_KEY = constants.PARAMETER_CONSTANT_START_TIMESTAMP_KEY\n",
    "    KEEP_FIRST_FOLLOWING = \"keep_first_following\"\n",
    "#dataPath = \"E:\\Data\\DOI-10-13012-b2idb-0647142_v3\\PhoenixBLN-NYT_1980-2018.csv\"\n",
    "dataPath = \"/media/sasha/HDD-bigboy/Data/DOI-10-13012-b2idb-0647142_v3/PhoenixBLN-NYT_1980-2018.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code2text_Pheonix(event_log):\n",
    "    #translatets root_codes for pheonix data\n",
    "    translationMap = {\"1\":\"Make Public Statement\", \"2\":\"Appeal\", \"3\":\"Express intent to cooperate\",\n",
    "                    \"4\":\"Consult\", \"5\":\"Engage in diplomatic cooperation\",\"6\":\"Engage in material cooperation\",\n",
    "                    \"7\":\"Provide aid\", \"8\":\"Yield\", \"9\":\"Investigate\", \n",
    "                    \"10\":\"Demand\", \"11\":\"Disapprove\", \"12\":\"Reject\", \n",
    "                    \"13\":\"Threaten\", \"14\":\"Protest\", \"15\":\"Exhibit force posture\", \n",
    "                    \"16\":\"Reduce relations\", \"17\":\"Coerce\", \"18\":\"Assault\",\n",
    "                    \"19\":\"Fight\",\"20\":\"Use unconventional mass violence\"}\n",
    "    def translate(x):\n",
    "        return translationMap[str(x)]\n",
    "    event_log['root_code_text'] = event_log['root_code'].apply(translate)    \n",
    "    return event_log\n",
    "\n",
    "def preprocess(event_log, key=\"empty\", remove_activity_keys=[], start_date=\"1780-01-01\", end_date=\"1780-01-01\"):\n",
    "    start_date += \" 00:00:00\"\n",
    "    end_date += \" 00:00:00\"\n",
    "    #end_date = pandas.to_datetime(end_date, utc=True)\n",
    "\n",
    "    #remove activities\n",
    "    if len(remove_activity_keys) > 0:\n",
    "        for code_key in remove_activity_keys:\n",
    "            event_log = event_log[event_log.root_code != code_key]\n",
    "            print(event_log[\"root_code\"])\n",
    "            \n",
    "    #extract root country\n",
    "    event_log[\"source_root\"] = event_log[\"source\"].apply(lambda x : x[0:3])\n",
    "    event_log[\"target_root\"] = event_log[\"target\"].apply(lambda x : x[0:3])\n",
    "    #create conflict_id\n",
    "    event_log[\"conflict_id\"] = event_log[\"source_root\"] + event_log[\"target_root\"]\n",
    "    #select conflict\n",
    "    if key != \"empty\":\n",
    "        event_log = event_log[event_log.conflict_id == key]\n",
    "    \n",
    "\n",
    "    #event_log = event_log[event_log.source == \"DDR\"]\n",
    "    #decode codes\n",
    "    event_log = code2text_Pheonix(event_log)\n",
    "    \n",
    "    #format to pm4py\n",
    "    event_log = pm4py.format_dataframe(event_log, case_id='conflict_id', activity_key='root_code_text', timestamp_key='story_date')\n",
    "    #select date range if needed\n",
    "    if start_date != end_date:\n",
    "        event_log = pm4py.filter_time_range(event_log, start_date, end_date)\n",
    "    \n",
    "    start_activities = pm4py.get_start_activities(event_log)\n",
    "    end_activities = pm4py.get_end_activities(event_log)\n",
    "    print(\"Start activities: {}\\nEnd activities: {}\".format(start_activities, end_activities))\n",
    "    return event_log\n",
    "\n",
    "def import_csv(file_path):\n",
    "    event_log = pandas.read_csv(file_path, sep=',')\n",
    "    return event_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_dfg(dfg):\n",
    "    #normalize the edges to sum to 1\n",
    "\n",
    "    absolut_weight = 0\n",
    "    for weight in dfg.values():\n",
    "        absolut_weight += weight\n",
    "    # normalize the weight of transitions into each target node\n",
    "    for key, weight in dfg.items():\n",
    "        dfg[key] = weight / absolut_weight\n",
    "\n",
    "    return dfg\n",
    "\n",
    "def target_normalize_dfg(dfg):\n",
    "    #normalize the edges to each target node to sum to 1\n",
    "    #note im using the origional dfg here so when we max_path_denoise\n",
    "    #so we can see how much percent of the incoming transitions each transition represents\n",
    "\n",
    "    # create a dictionary to store the total weight of transitions into each target node\n",
    "    total_weight = {}\n",
    "    absolut_weight = 0\n",
    "    for (source, target), weight in dfg.items():\n",
    "        absolut_weight += weight\n",
    "        if target in total_weight:\n",
    "            total_weight[target] += weight\n",
    "        else:\n",
    "            total_weight[target] = weight\n",
    "\n",
    "    # normalize the weight of transitions into each target node\n",
    "    for (source, target), weight in dfg.items():\n",
    "        dfg[(source, target)] = (weight / total_weight[target]) * absolut_weight\n",
    "\n",
    "    return dfg \n",
    "\n",
    "def source_normalize_dfg(dfg):\n",
    "    #normalize the edges to each source node to sum to 1\n",
    "    #note im using the origional dfg here so when we max_path_denoise\n",
    "    #so we can see how much percent of the incoming transitions each transition represents\n",
    "\n",
    "    # create a dictionary to store the total weight of transitions into each source node\n",
    "    total_weight = {}\n",
    "    for (source, target), weight in dfg.items():\n",
    "        if source in total_weight:\n",
    "            total_weight[source] += weight\n",
    "        else:\n",
    "            total_weight[source] = weight\n",
    "\n",
    "    # normalize the weight of transitions into each source node\n",
    "    for (source, target), weight in dfg.items():\n",
    "        dfg[(source, target)] = (weight / total_weight[source]) * absolut_weight\n",
    "\n",
    "    return dfg\n",
    "\n",
    "def scale_dfg(dfg):\n",
    "    l = 0\n",
    "    for val in dfg.values():\n",
    "        l += val\n",
    "    for key in dfg.keys():\n",
    "        dfg[key] *= (1/l)\n",
    "    return dfg\n",
    "\n",
    "def minthreshold_dfg(dfg, threshold):\n",
    "    return {activities: value for activities, value in dfg.items() if value >= threshold}\n",
    "\n",
    "def max_path_denoise_dfg(dfg):\n",
    "    return {activities: value for activities, value in dfg.items() \n",
    "               if activities[0] != activities[1] and \n",
    "               (value >= max([x for k,x in dfg.items() if k[0] == activities[0]])\n",
    "                or value >= max([x for k,x in dfg.items() if k[1] == activities[1]]))}\n",
    "\n",
    "def get_eventually_follows_in_days_graph(event_log, daysFollow):\n",
    "    \n",
    "    \n",
    "    ret_dict = {}\n",
    "    parameters = {}\n",
    "    event_log = converter.apply(event_log, variant=converter.Variants.TO_EVENT_LOG, parameters=parameters)\n",
    "    activity_key = exec_utils.get_param_value(Parameters.ACTIVITY_KEY, parameters, xes_constants.DEFAULT_NAME_KEY)\n",
    "    timestamp_key = exec_utils.get_param_value(Parameters.TIMESTAMP_KEY, parameters, xes_constants.DEFAULT_TIMESTAMP_KEY)\n",
    "    start_timestamp_key = exec_utils.get_param_value(Parameters.START_TIMESTAMP_KEY, parameters, xes_constants.DEFAULT_TIMESTAMP_KEY)\n",
    "    keep_first_following = exec_utils.get_param_value(Parameters.KEEP_FIRST_FOLLOWING, parameters, False)\n",
    "    time_threshold = pandas.Timedelta(days=daysFollow)\n",
    "\n",
    "    for trace in event_log:\n",
    "        sorted_trace = sorting.sort_timestamp_trace(trace, start_timestamp_key)\n",
    "        i = 0\n",
    "        while i < len(sorted_trace):\n",
    "            act1 = sorted_trace[i][activity_key]\n",
    "            tc1 = sorted_trace[i][timestamp_key]\n",
    "            j = i + 1\n",
    "            while j < len(sorted_trace):\n",
    "                ts2 = sorted_trace[j][timestamp_key]\n",
    "                act2 = sorted_trace[j][activity_key]\n",
    "                if ts2 - tc1 <= time_threshold and ts2 >= tc1:\n",
    "                    tup = (act1, act2)\n",
    "                    if tup not in ret_dict:\n",
    "                        ret_dict[tup] = 0\n",
    "                    ret_dict[tup] += 1\n",
    "                    if keep_first_following:\n",
    "                        break\n",
    "                j += 1\n",
    "            i += 1\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_process_tree(event_log, show=True):\n",
    "    process_tree = pm4py.discover_process_tree_inductive(event_log)\n",
    "    bpmn_model = pm4py.convert_to_bpmn(process_tree)\n",
    "    if(show):\n",
    "        pm4py.view_bpmn(bpmn_model)\n",
    "    return process_tree\n",
    "\n",
    "\n",
    "\n",
    "def make_efg(event_log, show=True, daysFollow=3, scale =False, normalize = False, normalize_to = \"target\", minThresh = 0., max_path_denoise=False):\n",
    "    efg = get_eventually_follows_in_days_graph(event_log, daysFollow)\n",
    "    \n",
    "    if normalize:\n",
    "        efg = normalize_dfg_to(efg, normalize_to)\n",
    "\n",
    "    if max_path_denoise:\n",
    "        efg = max_path_denoise_dfg(efg)      \n",
    "\n",
    "    if scale:\n",
    "        efg = scale_dfg(efg)\n",
    "    if minThresh > 0.:\n",
    "        efg = minthreshold_dfg(efg, minThresh)\n",
    "\n",
    "    if show:\n",
    "        pm4py.view_dfg(efg, start_activities=None, end_activities=None, format='png')\n",
    "    return efg\n",
    "\n",
    "\n",
    "\n",
    "def normalize_dfg_to(dfg, normalize_to = \"source\"):\n",
    "    if normalize_to == \"source\":\n",
    "        return source_normalize_dfg(dfg)\n",
    "    elif normalize_to == \"target\":\n",
    "        return target_normalize_dfg(dfg)\n",
    "    else:\n",
    "        return normalize_dfg(dfg)\n",
    "    \n",
    "\n",
    "\n",
    "def make_dfg(event_log, minThresh = 1,\n",
    "            max_path_denoise=False,\n",
    "            show=True, scale =False,\n",
    "            normalize = False, normalize_to = \"target\"):\n",
    "    \n",
    "    dfg, start_activities, end_activities = pm4py.discover_dfg(event_log)\n",
    "    \n",
    "    if normalize:\n",
    "        dfg = normalize_dfg_to(dfg, normalize_to)\n",
    "\n",
    "    if max_path_denoise:\n",
    "        dfg = max_path_denoise_dfg(dfg)      \n",
    "\n",
    "    if scale:\n",
    "        dfg = scale_dfg(dfg)\n",
    "\n",
    "    dfg = minthreshold_dfg(dfg, minThresh)\n",
    "\n",
    "    if show:\n",
    "        pm4py.view_dfg(dfg, start_activities=None, end_activities=None, format='png')\n",
    "    return dfg\n",
    "\n",
    "def make_petrinet(event_log, show=True):\n",
    "    #no clue what markings are but this should get us a petrinet\n",
    "    #network, iMarking, fMarking = pm4py.discovery.discover_petri_net_alpha(event_log)\n",
    "    #network, iMarking, fMarking = pm4py.discover_petri_net_inductive(event_log,noise_threshold = 0.7)\n",
    "    network, iMarking, fMarking = pm4py.discover_petri_net_heuristics(event_log)\n",
    "    if show:\n",
    "        pm4py.view_petri_net(network, iMarking, fMarking)\n",
    "    return network, iMarking, fMarking\n",
    "def make_bpmn(event_log, show=True):\n",
    "    efg = pm4py.discover_bpmn_inductive(event_log, noise_threshold = 0.7, multi_processing=True)\n",
    "    if show:\n",
    "        pm4py.view_bpmn(efg)\n",
    "    return efg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/sasha/HDD-bigboy/Data/DOI-10-13012-b2idb-0647142_v3/PhoenixBLN-NYT_1980-2018.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sasha\\OneDrive\\Documents\\Geopolitics-Process-Mining\\ProcMingNotebookTest.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sasha/OneDrive/Documents/Geopolitics-Process-Mining/ProcMingNotebookTest.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#selector block, config and select stuff here\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sasha/OneDrive/Documents/Geopolitics-Process-Mining/ProcMingNotebookTest.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m event_log \u001b[39m=\u001b[39m import_csv(dataPath)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sasha/OneDrive/Documents/Geopolitics-Process-Mining/ProcMingNotebookTest.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m event_log \u001b[39m=\u001b[39m preprocess(event_log,key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDEUDEU\u001b[39m\u001b[39m\"\u001b[39m, start_date\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1960-01-01\u001b[39m\u001b[39m\"\u001b[39m, end_date\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m2000-01-01\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sasha/OneDrive/Documents/Geopolitics-Process-Mining/ProcMingNotebookTest.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(event_log)\n",
      "\u001b[1;32mc:\\Users\\sasha\\OneDrive\\Documents\\Geopolitics-Process-Mining\\ProcMingNotebookTest.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sasha/OneDrive/Documents/Geopolitics-Process-Mining/ProcMingNotebookTest.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimport_csv\u001b[39m(file_path):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sasha/OneDrive/Documents/Geopolitics-Process-Mining/ProcMingNotebookTest.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     event_log \u001b[39m=\u001b[39m pandas\u001b[39m.\u001b[39;49mread_csv(file_path, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sasha/OneDrive/Documents/Geopolitics-Process-Mining/ProcMingNotebookTest.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m event_log\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/sasha/HDD-bigboy/Data/DOI-10-13012-b2idb-0647142_v3/PhoenixBLN-NYT_1980-2018.csv'"
     ]
    }
   ],
   "source": [
    "#selector block, config and select stuff here\n",
    "\n",
    "event_log = import_csv(dataPath)\n",
    "event_log = preprocess(event_log,key=\"DEUDEU\", start_date=\"1960-01-01\", end_date=\"2000-01-01\")\n",
    "\n",
    "print(event_log)\n",
    "\n",
    "#example dfg call\n",
    "dfg = make_dfg(event_log, scale=True,\n",
    "            max_path_denoise=False, minThresh=0.02, show=True, \n",
    "            normalize=False, normalize_to=\"source\")\n",
    "#example efg call\n",
    "efg = make_efg(event_log, scale=True,\n",
    "            max_path_denoise=False, minThresh=0.02, show=True, \n",
    "            normalize=False, normalize_to=\"source\", daysFollow=3)\n",
    "#example manual dfg construction\n",
    "dfg = pm4py.discover_dfg(event_log) #for efg; efg = get_eventually_follows_in_days_graph(event_log, daysFollow)\n",
    "dfg = normalize_dfg_to(dfg, normalize_to=\"source\")\n",
    "dfg = scale_dfg(dfg)\n",
    "dfg = minthreshold_dfg(dfg, 0.2)\n",
    "pm4py.show_dfg(dfg, start_activities=None, end_activities=None, format='png')\n",
    "\n",
    "print(dfg)\n",
    "\n",
    "#make_dfg(event_log_r,minThresh =4)\n",
    "#make_bpmn(event_log)\n",
    "#make_process_tree(event_log)\n",
    "#make_petrinet(event_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
